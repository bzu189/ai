{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_IMoqZWoyED",
        "outputId": "00c484b5-965c-4a7b-e2f2-c96493fed879"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Cost Function\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Computes the Mean Squared Error (MSE).\n",
        "    \"\"\"\n",
        "    m = len(Y)\n",
        "    predictions = np.dot(X, W)\n",
        "    cost = (1 / (2 * m)) * np.sum((predictions - Y) ** 2)\n",
        "    return cost\n",
        "\n",
        "# Gradient Descent\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Performs gradient descent to optimize the weights.\n",
        "    \"\"\"\n",
        "    m = len(Y)\n",
        "    cost_history = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        predictions = np.dot(X, W)\n",
        "        loss = predictions - Y\n",
        "        gradient = (1 / m) * np.dot(X.T, loss)\n",
        "        W = W - alpha * gradient\n",
        "        cost = cost_function(X, Y, W)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "# RMSE Calculation\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Computes Root Mean Squared Error.\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "\n",
        "# R-Squared Calculation\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Computes the R-squared value.\n",
        "    \"\"\"\n",
        "    ss_tot = np.sum((Y - np.mean(Y)) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/drive/MyDrive/DAV_SarahMaharjan/student.csv')\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values  # Target: Writing marks\n",
        "\n",
        "    # Add a bias (intercept) column to X\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))  # Adding a column of ones for intercept\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W), learning rate, and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
        "    alpha = 0.0001  # Learning rate\n",
        "    iterations = 1000  # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "WlfvyIrj3oeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2803c1-5f1a-430a-ebff-1a19dc8c6555"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.00221962 0.08948349 0.89502744]\n",
            "Cost History (First 10 iterations): [17.80549271459724, 16.983188130762354, 16.925187801661103, 16.867921576632344, 16.81114744297991, 16.75486109935263, 16.699058354723682, 16.643735054097604, 16.588887078177933, 16.53451034306046]\n",
            "RMSE on Test Set: 4.79263247324407\n",
            "R-Squared on Test Set: 0.908239378711656\n"
          ]
        }
      ]
    }
  ]
}